---
title: 'Ziqi Zhao CSCI E-63C: Final Exam'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

For the final exam/project we will develop classification models using several approaches and compare their performance on a new dataset that is a subset of one of the datasets used in machine learning common task framework (CTF) competitions.  A copy of the dataset is available in the zip-archive on our course website in Canvas as two separate files: training data `final-data-train.csv` (with the outcome variable `response` available) and test data `final-data-test.csv` (stripped of the outcome, for prediction submission purposes only).

The "training" dataset (the only one that has outcomes available) is for training your models *including* all the cross-validations and test error estimates as requested and/or as you may see fit. In that sense, those "training" data are the *full* and *only* dataset available to you.

The "test" dataset has *no* outcomes provided to you. We are the only ones who have those. After you come up with your best model(s) using the "training" data provided to you, you have to make predictions for this mystery "test" set and *submit your predictions*. We will evaluate the accuracy of your predictions and let you know what accuracy scores you achieved and also how you stand against your peers to make things a bit more fun. This is how data science competitions are often run (on platforms like Kaggle, in particular).

Thus, with this final exam/project you will be asked, in addition to the Rmarkdown and HTML files showing all your work, to also upload your *predictions on the test set* into Canvas as well, as separate CSV file(s).  

**IMPORTANT**: The file with predictions for the test dataset 

1. **must have** *two* columns, *comma-separated*, one row per each observation in the *test* dataset. 
2. The first column must contain the observation identifier (the value from column `id` in test dataset). 
3. The second column must contain the predictions of your model for observations in the test dataset (corresponding to the values of observation 'id' in the first column, or course). 
4. The predictions *must* be specified as "Y"/"N" labels. 
5. Finally, the CSV file must contain a header line, please label the first column 'id', and the second column (predictions) label can be anything you like - this will be the name of your model which will appear in the leaderboard (you don't have to put your name there, you can stay anonymous to your peers and call it 'myCoolModel3').

The submissions that do not follow that exact format will be rejected *without* generating error messages or sending automatic followup emails  (we will try to send those as a *courtesy* if we do notice some problems with the submission format). 

For your reference, you can examine a few examples of test prediction files following this format provided to you in the same zip archive you downloaded from the Canvas page (`predictions-*.csv` files in the `predictions-examples` sub-folder).

One more time, just to re-iterate and to emphasize, please notice that this time your final submission must consist of the following *three* (not just two, Rmd+html, as usual) items:

* Rmarkdown *.Rmd file with all the calculations you want to receive credit for,
* HTML version of the output generated by your *.Rmd file, and
* Final **predictions** for the **test** dataset in comma-separated values (CSV) format from the best model you could come up with (file name must have *.csv extension for the file to load in Canvas).

While you are working on improving your models you are invited to upload your predictions (following the same CSV format as described above) into Canvas repeatedly. For those trial submissions, you do not need to upload your intermediate RMDs or HTMLs (we will grade *only* those supplied in your *final* submission anyway). Just the CSV file, with predictions for the test set. You *can* submit multiple CSV files at once. But please be reasonable: if you have 2-3 highly competitive models that you like, feel free to submit their predictions together, to test them all against the mystery test set. But if you are submitting dozens of predictions obtained, for instance, over a set of hyperparameters, then you are essentially starting explicitly tuning your model against the test set, which is not fair: in this case it is not a test set anymore, it becomes a part of your training set!!

We will download the current submissions at least daily and generate a leaderboard sorted by prediction accuracy computed against the true values of the outcome for the test dataset (we will also show sensitivity, specificity, etc.).  This list will be made available on our course website in Canvas for everyone in this class in order to see how the performance of their models compares to the models built by other students in the class. 

Please note that if you make multiple separate submissions (e.g. every day, or within a single day before the leaderboard is updated), it is *only your latest submission* that is used to rebuild the leaderboard. As mentioned above, if you want to evaluate a couple of models, please make those CSV files a *single* submission.

The first version of the leaderboard posted on the course website at the time when final exam is made available starts with predictions provided in those few example CSV files available in the zip-archive (coin flip, majority vote, etc. -- what do you think Charlie Brown is using to make the predictions?).  Those should be pretty easy to improve upon.

It is 100% up to you whether you want to upload your model predictions over the course of the week, how frequently you want to do it and what you want its results to be called in the leaderboard posted for everyone in the class to see. Again, we will use the name of the 2nd column in the prediction file as the model name listed in the leaderboard.  You will not get extra points for those interim submissions (but hopefully it will make the project more fun). Onl y when you are done with the final, you *have* to load predictions from your best model into Canvas alongside the HTML and RMD files (again, make all those a *single* submission).

**IMPORTANT:** the dataset is noticeably larger and correspondingly some of the modeling techniques may and will run slower than what we were typically encountering in this class. This is a more realistic scenario and as the graduates of this course you are expected to be able to address those issues. You may find it extremely helpful to do at least some of the initial exploration, model building and tuning, code debugging etc on multiple random samples of smaller size as you decide on useful ranges of parameters/modeling choices. When you feel ready, perform a final run of fully debugged and working code on the full training dataset.  Please see also the afterword to this assignment.

# Problem 1: univariate and unsupervised analysis (points: 20/20+extra 7 - graduate/undergraduate)

1. (points: 3/6) Download and read training and test data into R and prepare graphical and numerical summaries of it: e.g. histograms of continuous attributes, contingency tables of categorical variables, scatterplots of continuous attributes with some of the categorical variables indicated by color/symbol shape, etc.  Whatever you find helpful to think about properties of the data you are about to start using for fitting classification models.

As it is often the case for such contests, the attributes in the dataset are blinded in the sense that no information is available about what those are or what their values mean.  The only information available is that the attribute `response` is the outcome to be modeled and the attribute `id` is the unique numerical identifier for each observation.  Some of the remaining attributes are clearly categorical (those that are character valued) and some rather obviously continuous (those with numerical values with large number of unique values).  For several of them it is less clear whether it is best to treat them as continuous or categorical -- e.g. their values are numerical but there are relatively few unique values with many observations taking the same value, so that they arguably could be treated as either continuous or categorical. Their relationships to other variables and to the outcome observed through the initial data exploration might guide the choice. 

2. (points: 2/2 extra) Please identify those variables of questionable type, reflect on how you prefer to handle them and describe this in your own words. 

3. (points: 3/4) Perform principal components analysis of the data (do you need to scale it prior to that? how would you represent multilevel categorical attributes to be used as inputs for PCA?) and 
4. (points: 3/5) plot observations in the space of the first few principal components indicating levels of some of the categorical attributes of your choosing by the color/shape of the symbol.  
5. (points: 5/5 extra) Perform univariate assessment of associations between the outcome we will be modeling and each of the attributes (e.g. t-test or logistic regression for continuous attributes, contingency tables/Fisher exact test/$\chi^2$ test for categorical attributes).  
6. (points: 4/5) Summarize your observations from these univariate assessments: does it appear that there are predictors associated with the outcome `response` univariately (i.e. when considered on their own)? Which predictors seem to be more/less relevant?



First, a note that you will see the train data being repeatedly read in and re-split and preprocessed in most if not each problem. This was done intentionally by me as for different built-in methods the requirement on how the data was to be processed may be slightly different. I may have also ran smaller samples of the data to answer each problem to avoid long runtimes. To avoid errors (I initially had many) with the train data and having to change it slightly and keep track of it, which is rather messy, I decided to just re-read and preprocess the data repeatedly, which did not impact runtime in any significant way.

Also note: "N" is the positive class for all models given it is overpresented in the train data at c.75% of all response points.

```{R load libraries}
# Load necessary libraries
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(tidyr)
library(tidyverse)
library(caret)
library(MASS)
library(randomForest)
library(caret)
library(pROC)
library(e1071)
library(nnet)
library(reshape2)
library(dplyr)
```


```{R P1.1}

data<- read.csv("final-data-train.csv")

# Split the data into training and testing sets (80/20 split)
trainIndex <- createDataPartition(data$response, p = 0.8, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Standardize levels in both train and test data (default gave Nulls)
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data$response <- factor(test_data$response, levels = c("N", "Y"))

# Identify categorical and continuous variables
categorical_vars <- names(train_data)[sapply(train_data, is.character)]
continuous_vars <- names(train_data)[sapply(train_data, is.numeric)]

# A numerical version of the response levels required later in logistic regression, etc.
train_data$response_binary <- ifelse(train_data$response == "Y", 1, 0)
test_data$response_binary <- ifelse(test_data$response == "Y", 1, 0)

# Summary outputs
str(train_data)
summary(train_data)
cor_matrix <- cor(train_data[continuous_vars])
print(cor_matrix)

```

```{R P1.2}

# Histograms for continuous variables
for (var in continuous_vars) {
  print(ggplot(train_data, aes(x = .data[[var]])) +
    geom_histogram(bins = 30, fill = "blue", alpha = 0.5) +
    labs(title = paste("Histogram of", var), x = var, y = "Frequency") +
    theme_minimal())
}

# Bar plots for categorical variables
for (var in categorical_vars) {
  print(ggplot(train_data, aes_string(x = var)) +
    geom_bar(fill = "blue", alpha = 0.5) +
    labs(title = paste("Bar plot of", var), x = var, y = "Count") +
    theme_minimal())
}
```
The output, histograms and bar charts gives us some insights to the data and variables. 

The data itself seem clean (No mixed variable columns, no missing values, etc.).

First, we see that the response distribution is skewed, with around 3/4 being "N" and only around 1/4 being "Y".

Overall, the numerical variables look to be on a similar scale of magnitude, in the single digits with max values < 20, thus no pre-scaling will be done (All except "id", which is unique and I will leave it as is).

From the bar plots, the categorical variables do not have an excessive number of categories and do not need to be adjusted to continuous.

From the correlation matrix, no obviously highly correlated numeric predictors exist, no obvious multicollinearity.

No special outliers or unique levels has been observed and thus no special pre-processing is required.

```{R P1.25}
# Ensure response is a factor for color
train_data$response <- as.factor(train_data$response)

# Scatterplot for first two continuous variables, seems to show no relationship with response, and thus not going to try all pairs here
ggplot(train_data, aes(x = .data[[continuous_vars[1]]], y = .data[[continuous_vars[2]]], color = response)) +
  geom_point(alpha = 0.7) +
  labs(title = "Scatterplot of first two continuous variables") +
  theme_minimal()

# Scatterplot for first two continuous variables, seems to show no relationship with response, and thus not going to try all pairs here
ggplot(train_data, aes(x = .data[[continuous_vars[3]]], y = .data[[continuous_vars[4]]], color = response)) +
  geom_point(alpha = 0.7) +
  labs(title = "Scatterplot of first two continuous variables") +
  theme_minimal()

# Scatterplot for first two continuous variables, seems to show no relationship with response, and thus not going to try all pairs here
ggplot(train_data, aes(x = .data[[continuous_vars[5]]], y = .data[[continuous_vars[6]]], color = response)) +
  geom_point(alpha = 0.7) +
  labs(title = "Scatterplot of first two continuous variables") +
  theme_minimal()

```
I show a scatterplot of id vs dtj with responses:We see id being evenly spread and dtj being clearly skewed in distribution. No clear pattern of the response can be seen in this dimension. 

The next two variables shown are sci vs qh, we see a clear separation of the response based on qh, so the predictive power of qh can already been seen visually.

Finally, bw vs ypz is shown. We see clearly there are only less than 20 unique ypz values, the response do not seem to correlate very strongly with either predictor, perhaps bw has some predictive power with higher values showing more "Y".


Not showing all scatterplots using "pairs" here as it may be too hard to read and will add it later if need be (If I run into problems later and need more insight on the data).



Answer to 2: Variables that are numerical but have a relatively small number of distinct values (for example dtj and ypz as shown in the histograms only have 15-20 distinct values) may appear to be continuous but could arguably be treated as categorical.

Note for entire final: However, I have decided not to change dtj and ypz for the purpose of later analysis and method applications. My general principle for this final is to change the data as little as possible to begin with, i.e. No transformations, I will also include the column "id" as a predictor since it is unique and does not seem obviously random (not an ordered list starting from 1). No exclusions of variables unless it is systematically shown to be an improvement (i.e. Multicollinearity is found).

Unless the results are not satisfactory I will not tamper with the given data, and we will see later that the metrics turn out just fine without any changes.


```{R P1.3}

# Numeric and categorical data separated
numeric_data <- train_data[, continuous_vars]
categorical_data <- train_data[, categorical_vars]

# Remove the 'response' column from categorical_data if it exists
categorical_data <- categorical_data[, !(colnames(categorical_data) %in% "response")]

# Create dummy variables for the categorical data
categorical_data_encoded <- model.matrix(~ . - 1, data = categorical_data)  # -1 removes intercept

# Combine the scaled numeric data and the dummy variables
numeric_data_scaled <- scale(numeric_data)  # Scale numeric data
combined_data <- cbind(numeric_data_scaled, categorical_data_encoded)

# Perform PCA on the combined data (scaled numeric + encoded categorical)
pca_result <- prcomp(combined_data, center = TRUE, scale. = TRUE)

# Check if the response variable exists and is a factor
if ("response" %in% names(train_data)) {
  train_data$response <- as.factor(train_data$response)
  
  # Ensure row names in PCA align with `response`
  rownames(combined_data) <- rownames(train_data)

  # Visualize PCA with response variable
  fviz_pca_ind(pca_result,
               geom = "point",
               habillage = train_data$response,  # Use 'response' variable to color points
               addEllipses = TRUE,
               palette = "jco")  
} else {
  
  # Visualize PCA without response variable
  fviz_pca_ind(pca_result,
               geom = "point",
               addEllipses = TRUE,
               palette = "jco")
}

```
Scaling is essential here prior to pca as it is sensitive to variable variances, which could be on different scales (In my case "id" was not scaled earlier but much larger), so we scale the data here first.
For the categorical variables, I have created dummy variables here as inputs to pca since pca requires numerical data.

Interpreting the results of the intial plot, the groups seem to be separated along the first two principal components (PC1 and PC2). The colors and shapes represent different levels of response and we see this defined fairly clearly. The clear separation suggests that the variables driving this separation has a significant correlation with variance, are influential and can be leveraged for classification (i.e. The blinded data is not randomly put together for an April fools prank). The response categories are well-defined in the lower-dimensional space, meaning PCA is effectively capturing the underlying structure of the data.

```{R 1.4}

# Extract the first two principal components
pca_scores <- pca_result$x[, 1:2]  # First two principal components

# Create a data frame combining PCA results and the categorical variables (including 'response')
pca_df <- data.frame(pca_scores)
pca_df$response <- train_data$response  # Add the response variable for coloring
pca_df$np <- train_data$np  # Add another categorical variable for shape

# Plot the observations in PCA space (first two components)
ggplot(pca_df, aes(x = PC1, y = PC2, color = response, shape = train_data$np)) +
  geom_point(size = 3) +
  labs(title = "PCA: First Two Principal Components with response and np",
       x = "PC1", y = "PC2") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue")) +  # Adjust colors as needed
  scale_shape_manual(values = c(16, 17, 18))  # Adjust shape codes for different categories

```
np seem to be somewhat correlated with response.

The spread along PC1 and PC2 reflects how well the variance in the data is captured by these two components. If most of the variance is captured by PC1, then the spread along the x-axis (PC1) will be more significant. This could suggest that PC1 explains the majority of the variability in the dataset.

```{R 1.5}

pca_df$xt <- train_data$xt  # Add another categorical variable for shape

# Plot the observations in PCA space (first two components)
ggplot(pca_df, aes(x = PC1, y = PC2, color = response, shape = train_data$xt)) +
  geom_point(size = 3) +
  labs(title = "PCA: First Two Principal Components with response and xt",
       x = "PC1", y = "PC2") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue")) +  # Adjust colors as needed
  scale_shape_manual(values = c(16, 17, 18, 22, 25))  # Adjust shape codes for different categories

```
xt categories also seem to have some correlation with response, e.g. ntb is skewed towards "Y", etc. Though not perfectly and not all categories are easy to see with the eye on this plot. That is it for now as too many category levels become hard to visually interpret. 

```{R P1.6}
# Logistic regression for continuous variables
logistic_results <- list()
for (var in continuous_vars) {
  formula <- as.formula(paste("response_binary ~", var))
  model <- glm(formula, data = train_data, family = "binomial")
  logistic_results[[var]] <- summary(model)
}

# Print the logistic regression summaries
for (var in names(logistic_results)) {
  cat("\nResults for", var, ":\n")
  print(logistic_results[[var]])
}

# Chi-squared tests for categorical variables
chi_squared_results <- list()
for (var in categorical_vars) {
  table_var <- table(train_data[[var]], train_data$response)
  chi_squared_results[[var]] <- chisq.test(table_var)
}

# Summarize significant predictors
significant_vars <- c()
for (var in continuous_vars) {
  if (logistic_results[[var]]$coefficients[2, 4] < 0.05) {
    significant_vars <- c(significant_vars, var)
  }
}
for (var in categorical_vars) {
  if (chi_squared_results[[var]]$p.value < 0.05) {
    significant_vars <- c(significant_vars, var)
  }
}

# Overall result summaries

cat("\nContinuous variables showed significant associations, including:")
print(continuous_vars[continuous_vars %in% significant_vars])

cat("\nCategorical variables showed significant associations, including:")
print(categorical_vars[categorical_vars %in% significant_vars])

cat("\nOverall, the following variables were significantly associated with the outcome:\n")
print(significant_vars)

```
Given the univariate logistic and chi-squared tests above, almost all the variables are significant predictors at the 95% level. We observe some significant predictors such as is to have a larger p-value compared to other predictors.

"id" is shown to not be a significant predictor per univariate logistic regression, but I will still keep it for the later methods that are more robust to high dimensionality.

Yes, there appears to be predictors associated with the outcome `response` univariately, as we have seen this in the prior plots already. E.g. "qh". However, they are not systematically listed above as we would need to shown the confusion tables for each variable. 


# Problem 2: logistic regression (points: 20/20+extra 2 - graduate/undergraduate)

1. (points: 4/5) Develop logistic regression model of the outcome `response` as a function of multiple predictors in the model.  
2. (points: 2/2 extra) Which variables are significantly associated with the outcome? 
3. (points: 14/15) Test model performance on multiple splits of data into training and test subsets and summarize it in terms of accuracy/error/sensitivity/specificity.



```{R 2.1}
# Get all the predictor variables (excluding the response variable)
all_vars <- setdiff(names(train_data), c("response", "response_binary", "id"))

# Prepare the formula without response (use response_binary instead)
logistic_formula <- as.formula(paste("response_binary ~", paste(all_vars, collapse = " + ")))

# Fit the logistic regression model with the updated formula
logistic_model <- glm(logistic_formula, data = train_data, family = "binomial")

# Display the summary of the model
summary(logistic_model)

# Extract significant predictors (if needed)
significant_vars <- summary(logistic_model)$coefficients
significant_vars <- significant_vars[significant_vars[, "Pr(>|z|)"] < 0.05, , drop = FALSE]

# Print significant variables
cat("Significant variables:\n")
print(rownames(significant_vars))

```

**Discussions:**

The results are shown above, which includes categorical variable levels. All variables except the response and id are included.

For the continous variables, sci and bw are not significant at the 95% level, the others 4 are. (id is still not significant, I excluded it in the final run)

For the categorical levels, many are significant and we end up with around 35 significant variables in total as listed to be signficant at the 95% level.

```{R P2.2}

# Create a function to calculate sensitivity, specificity, and accuracy
evaluate_performance <- function(pred, truth) {
  conf_matrix <- confusionMatrix(as.factor(pred), as.factor(truth), positive = "0")
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  specificity <- conf_matrix$byClass["Specificity"]
  accuracy <- conf_matrix$overall["Accuracy"]
  error <- 1 - conf_matrix$overall["Accuracy"]  
  list(sensitivity = sensitivity, specificity = specificity, accuracy = accuracy, error = error)
}

# Get all the predictor variables (excluding the response variable)
all_vars <- setdiff(names(train_data), c("response", "response_binary", "id"))

# Prepare the formula (I used response_binary which converted N and Y into 0 and 1)
logistic_formula <- as.formula(paste("response_binary ~", paste(all_vars, collapse = " + ")))

# Set seed for reproducibility
set.seed(1234)

# Create an empty list to store results
performance_results <- list()

# Perform 10 splits
for (i in 1:10) {
  
  # Split the data into training and testing sets (80-20 split)
  train_idx <- createDataPartition(train_data$response_binary, p = 0.8, list = FALSE)
  train_split <- train_data[train_idx, ]
  test_split <- train_data[-train_idx, ]
  
  # Train the logistic regression model on the current training set
  logistic_model <- glm(logistic_formula, data = train_split, family = "binomial")
  
  # Make predictions on the test set
  test_prob <- predict(logistic_model, newdata = test_split, type = "response")
  test_pred <- ifelse(test_prob > 0.5, 1, 0)
  
  # Evaluate performance
  perf <- evaluate_performance(test_pred, test_split$response_binary)
  
  # Store the results
  performance_results[[i]] <- perf
}

# Summarize performance metrics
sensitivity <- sapply(performance_results, `[[`, "sensitivity")
specificity <- sapply(performance_results, `[[`, "specificity")
accuracy <- sapply(performance_results, `[[`, "accuracy")
error <- sapply(performance_results, `[[`, "error")

cat("Performance Metrics Summary (10 Splits):\n")
cat("Mean Sensitivity:", mean(sensitivity), "\n")
cat("Mean Specificity:", mean(specificity), "\n")
cat("Mean Accuracy:", mean(accuracy), "\n")
cat("Mean Error:", mean(error), "\n")

```
**Discussions:**

The Specificity is not great here with vanilla logistic regression at around 65%, but there is a class imbalance upon re-checking the proportions below with around 75% being "N" or "0". We also saw this in the barplots earlier. We try a weighted GLM instead.

```{R 2.3}
# Check for class imbalance
table(train_data$response)  # Check the counts of 0 and 1
prop.table(table(train_data$response))  # Check % proportions
```

```{R 2.4}
# Create a function to calculate sensitivity, specificity, and accuracy
evaluate_performance <- function(pred, truth) {
  conf_matrix <- confusionMatrix(as.factor(pred), as.factor(truth), positive = "1")
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  specificity <- conf_matrix$byClass["Specificity"]
  accuracy <- conf_matrix$overall["Accuracy"]
  error <- 1 - conf_matrix$overall["Accuracy"]  
  list(sensitivity = sensitivity, specificity = specificity, accuracy = accuracy, error = error)
}

# Get all the predictor variables (excluding the response variable)
all_vars <- setdiff(names(train_data), c("response", "response_binary", "id"))

# Prepare the formula (I used response_binary which converted N and Y into 0 and 1)
logistic_formula <- as.formula(paste("response_binary ~", paste(all_vars, collapse = " + ")))

# Set seed for reproducibility
set.seed(1234)

# Create an empty list to store results
performance_results <- list()

# Perform multiple splits (e.g., 10 splits)
for (i in 1:10) {
  # Split the data into training and testing sets (80-20 split)
  train_idx <- createDataPartition(train_data$response_binary, p = 0.8, list = FALSE)
  train_split <- train_data[train_idx, ]
  test_split <- train_data[-train_idx, ]
  
  # Assign weights inversely proportional to class frequencies in the current training set
  class_weights <- ifelse(train_split$response_binary == 1,
                          1 / sum(train_split$response_binary == 1),
                          1 / sum(train_split$response_binary == 0))
  
  # Train the weighted logistic regression model on the training set
  logistic_model_weighted <- glm(logistic_formula, data = train_split, family = "binomial", weights = class_weights)
  
  # Make predictions on the test set
  test_prob <- predict(logistic_model_weighted, newdata = test_split, type = "response")
  test_pred <- ifelse(test_prob > 0.5, 1, 0)
  
  # Evaluate performance
  perf <- evaluate_performance(test_pred, test_split$response_binary)
  
  # Store the results
  performance_results[[i]] <- perf
}

# Summarize performance metrics
weighted_logistic_sensitivity <- sapply(performance_results, `[[`, "sensitivity")
weighted_logistic_specificity <- sapply(performance_results, `[[`, "specificity")
weighted_logistic_accuracy <- sapply(performance_results, `[[`, "accuracy")
weighted_logistic_error <- sapply(performance_results, `[[`, "error")

# Print summary of performance metrics
cat("Performance Metrics Summary (10 Splits with Weighted Logistic Regression):\n")
cat("Mean Sensitivity:", mean(weighted_logistic_sensitivity), "\n")
cat("Mean Specificity:", mean(weighted_logistic_specificity), "\n")
cat("Mean Accuracy:", mean(weighted_logistic_accuracy), "\n")
cat("Mean Error:", mean(weighted_logistic_error), "\n")

```
**Discussions:**
Weighted GLM according to class frequencies improved was performed. Results are better according to the below with sensitivity now above 80%.

## Extra points problem: interaction terms (points: 5/5 - graduate/undergraduate)

Assess the impact/significance of pairwise interaction terms for all pairwise combinations of covariates (predictor variables) used in the model and report the top ten that most significantly improve model fit.

```{R Extra points 2.5}
```
Not done here due to excessive runtime encountered.

# Problem 3: linear discriminant analysis (points: 15/15 - graduate/undergraduate)

1. (points: 3/3) Fit linear discriminant analysis model of the outcome `response` as a function of the predictor variables in the dataset.  Feel free to decide whether you want to use all of them or a subset of those.  
2. (points: 5/5) Test resulting model performance on multiple splits of the data into training and test subsets, and 
3. (points: 7/7) summarize it in terms of accuracy/error/sensitivity/specificity and compare them to those obtained for logistic regression.


```{R P3.1}
# Re-reading in the data to refresh from prior splits
data<- read.csv("final-data-train.csv")

# Split the data into training and testing sets (80/20 split)
trainIndex <- createDataPartition(data$response, p = 0.8, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Standardize levels in both train and test data (by default is Null)
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data$response <- factor(test_data$response, levels = c("N", "Y"))

# Fit the LDA model using all predictors
lda_model <- lda(response ~ ., data = train_data)
summary(lda_model)

# Predict on test data
lda_pred <- predict(lda_model, newdata = test_data)

# Confusion Matrix for LDA predictions
conf_matrix_lda <- confusionMatrix(lda_pred$class, test_data$response)

# Output the confusion matrix and performance metrics for LDA
print(conf_matrix_lda)

# Summarize accuracy, error, sensitivity, specificity
lda_accuracy <- conf_matrix_lda$overall['Accuracy']
lda_error <- 1 - lda_accuracy
lda_sensitivity <- conf_matrix_lda$byClass['Sensitivity']
lda_specificity <- conf_matrix_lda$byClass['Specificity']

cat("LDA Model Performance:\n")
cat("Accuracy: ", lda_accuracy, "\n")
cat("Error: ", lda_error, "\n")
cat("Sensitivity: ", lda_sensitivity, "\n")
cat("Specificity: ", lda_specificity, "\n")

```

```{R 3.2}
num_splits <- 10

# Initialize a data frame to store the results
results <- data.frame(Accuracy = numeric(num_splits),
                      Error = numeric(num_splits),
                      Sensitivity = numeric(num_splits),
                      Specificity = numeric(num_splits))

# Loop through multiple splits
for (i in 1:num_splits) {
  # Split the data into training and testing sets (80/20 split)
  trainIndex <- createDataPartition(data$response, p = 0.8, list = FALSE)
  train_data <- data[trainIndex, ]
  test_data <- data[-trainIndex, ]

  # Standardize levels in both train and test data (by default is Null)
  train_data$response <- factor(train_data$response, levels = c("N", "Y"))
  test_data$response <- factor(test_data$response, levels = c("N", "Y"))

  # Fit the LDA model using all predictors in the dataset
  lda_model <- lda(response ~ ., data = train_data)
  
  # Predict on test data
  lda_pred <- predict(lda_model, newdata = test_data)
  
  # Confusion Matrix for LDA predictions
  conf_matrix_lda <- confusionMatrix(lda_pred$class, test_data$response)

  # Store the performance metrics
  results$Accuracy[i] <- conf_matrix_lda$overall['Accuracy']
  results$Error[i] <- 1 - conf_matrix_lda$overall['Accuracy']
  results$Sensitivity[i] <- conf_matrix_lda$byClass['Sensitivity']
  results$Specificity[i] <- conf_matrix_lda$byClass['Specificity']
}

# Calculate the average of each metric
avg_results_lda <- colMeans(results)

# Combine average results into a single summary
summary_results <- data.frame(Average = avg_results_lda)

# Print the summary results
print(summary_results)

```

```{R 3.3}

# Compare both models' performance by combining them into a single data frame:
comparison_metrics <- data.frame(
  Model = c("LDA", "Weighted Logistic Regression"),
  Accuracy = c(avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)
```
Weighted Logistic Regression is still better than LDA by a small margin, we see the weighted logistic regression has a worse sensitivity but much better specificity, which may be due to the imbalance in the train data response given and the weighted adjustment I did (hey it is a contest). Otherwise LDA would outperform vanilla logistic regression without the weight adjustment.

## Extra points problem: quadratic discriminant analysis (points: 5/5 - graduate/undergraduate)

In our experience attempting to fit quadratic discriminant analysis model of the categorical outcome `response` on this data results in a rank deficiency related error. Determine on how to correct this error and report resulting model training and test error/accuracy/etc. and how it compares to LDA and logistic regression above. 


```{R Extra Points 3.5}

# Standardize the continuous variables in the dataset
train_data_scaled <- train_data
train_data_scaled[ , continuous_vars] <- scale(train_data[ , continuous_vars])

# Perform PCA on the continuous variables to reduce dimensionality as a method of fixing rank deficiency
pca <- prcomp(train_data_scaled[ , continuous_vars], center = TRUE, scale. = TRUE)

# Apply PCA transformation to the training data
train_data_pca <- data.frame(pca$x)

# Add the response variable back to the PCA-transformed data
train_data_pca$response <- train_data$response

# Fit the QDA model on the PCA-transformed data
qda_model <- qda(response ~ ., data = train_data_pca)

# Scale the test data and apply PCA using the same parameters as the training data
test_data_scaled <- test_data
test_data_scaled[ , continuous_vars] <- scale(test_data[ , continuous_vars])

# Apply PCA transformation to the test data using the same PCA model (prcomp)
test_data_pca <- predict(pca, newdata = test_data_scaled[ , continuous_vars]) 
test_data_pca <- data.frame(test_data_pca)

# Add the response variable back to the test data after PCA transformation
test_data_pca$response <- test_data$response

# Predict using the QDA model
qda_pred <- predict(qda_model, newdata = test_data_pca)

# Confusion Matrix for QDA predictions
conf_matrix_qda <- confusionMatrix(qda_pred$class, test_data_pca$response)

# Output the QDA performance metrics
print(conf_matrix_qda)

# Store the performance metrics for comparison
qda_accuracy <- conf_matrix_qda$overall['Accuracy']
qda_error <- 1-qda_accuracy
qda_sensitivity <- conf_matrix_qda$byClass['Sensitivity']
qda_specificity <- conf_matrix_qda$byClass['Specificity']


# Print QDA model results
cat("QDA Model Performance:\n")
cat("Accuracy:", qda_accuracy, "\n")
cat("Error:", qda_error, "\n")
cat("Sensitivity:", qda_sensitivity, "\n")
cat("Specificity:", qda_specificity, "\n")

```

In the case of QDA, since it estimates separate covariance matrices for each class, a singular or near-singular covariance matrix can lead to problems, especially if some predictors are redundant or have low variance. PCA with scaling was used to correct this.

PCA was performed on the scaled continuous variables to reduce dimensionality and avoid rank deficiency. PCA transforms the data into a smaller set of orthogonal components that capture the maximum variance in the data. By selecting the principal components, we essentially reduced the potential multicollinearity and redundancy between features.

```{R 3.75}
#Compare all models' performance by combining them into a single data frame:
comparison_metrics <- data.frame(
  Model = c("QDA","LDA", "Weighted Logistic Regression"),
  Accuracy = c(qda_accuracy, avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(qda_error, avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(qda_sensitivity, avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(qda_specificity, avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)
```
Comparing our results obtained so far, a specificity of only slightly above 30% for QDA is terrible and false positives are rampant. Again this might be due to an imbalance in class distribution in the original train data. In any case, weighted logistic regression still performs best with LDA coming in at a close second.

# Problem 4: random forest (points: 15/15+extra 3 - graduate/undergraduate)

1. (points: 3/4) Develop random forest model of outcome `response`. 
2. (points: 3/3 extra) Present variable importance plots and comment on relative importance of different attributes in the model.  Did attributes showing up as more important in random forest model also appear as significantly associated with the outcome by logistic regression?  
3. (points: 4/5) Test model performance on multiple splits of data into training and test subsets and compare the "true" test error obtained in this way to the out-of-bag error estimates, 
4. (points: 5/6) summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of logistic regression and LDA models above.

```{R P4.1}

# Re-load the data for training
set.seed(42)
full_data_train <- read.csv("final-data-train.csv")

# Sample 3% of the full training data for the new train_data (Full data used for final run)
train_data_sample <- full_data_train
train_data_sample <- full_data_train[sample(nrow(full_data_train), size = 0.03 * nrow(full_data_train)), ]

# Split the train_data into a new training set and a new test set (80/20 split)
train_indices <- sample(nrow(train_data_sample), size = 0.8 * nrow(train_data_sample))  
train_data <- train_data_sample[train_indices, ]  
test_data <- train_data_sample[-train_indices, ]  

# Standardize levels in both train and test data (by default is Null)
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data$response <- factor(test_data$response, levels = c("N", "Y"))

# Fit Random Forest Model on the training data (excluding response_binary)
rf_model <- randomForest(response ~ ., 
                         data = train_data, 
                         ntree = 500, mtry = sqrt(ncol(train_data)-1), importance = TRUE)

print(rf_model)

# Evaluate model performance with confusion matrix
rf_pred <- predict(rf_model, newdata = test_data)
conf_matrix_rf <- confusionMatrix(rf_pred, test_data$response)
print(conf_matrix_rf)

```

```{R P4.2}
# Variable Importance Plot
varImpPlot(rf_model)

# Print variable importance
importance(rf_model)
```
We see variables like qh, sb, wi topping the importance plots, which is expected given their significance found earlier in univariate analysis. Some were even found to have univariate correlation with the response such as qh.

However there are variables of relatively high importance, most notably "sci", which was found to be not significant in the logistic regression analysis. This is plausible because random forest is very effective in capturing non-linear relationships, whereas logistic may not capture non-linear relationships as well. Random forest is not affected by multicollinearity in the same way as the data is split based on features in trees and the important plot ranks by impurity reduction and we are not looking at p-values.


```{R P4.3}
# Set the number of splits 
num_splits <- 10

# Initialize a data frame to store results
results <- data.frame(Accuracy = numeric(num_splits),
                      Error = numeric(num_splits),
                      Sensitivity = numeric(num_splits),
                      Specificity = numeric(num_splits),
                      OOB_Error = numeric(num_splits))

# Loop through multiple splits
for (i in 1:num_splits) {
  
  # Split the data into training and testing sets (80/20 split)
  train_indices <- sample(nrow(train_data_sample), size = 0.8 * nrow(train_data_sample))  # 80% for training
  train_data <- train_data_sample[train_indices, ]  # New training data
  test_data <- train_data_sample[-train_indices, ]  # New test data
  
  # Standardize levels in both train and test data
  train_data$response <- factor(train_data$response, levels = c("N", "Y"))
  test_data$response <- factor(test_data$response, levels = c("N", "Y"))
  
  # Fit Random Forest Model on the training data
  rf_model <- randomForest(response ~ ., 
                           data = train_data, 
                           ntree = 500, mtry = sqrt(ncol(train_data) - 1), importance = TRUE)
  
  # Evaluate Model Performance on Validation Data
  rf_pred <- predict(rf_model, newdata = test_data)
  
  # Ensure that the predicted values are factors
  rf_pred <- factor(rf_pred, levels = c("N", "Y"))
  
  # Confusion Matrix for Random Forest
  conf_matrix_rf <- confusionMatrix(rf_pred, test_data$response)
  
  # Store the performance metrics
  results$Accuracy[i] <- conf_matrix_rf$overall['Accuracy']
  results$Error[i] <- 1 - conf_matrix_rf$overall['Accuracy']
  results$Sensitivity[i] <- conf_matrix_rf$byClass['Sensitivity']
  results$Specificity[i] <- conf_matrix_rf$byClass['Specificity']
  
  # Store Out-of-Bag Error Estimate
  results$OOB_Error[i] <- rf_model$err.rate[500, 1]  # Get the OOB error for the final tree
  
}

# Calculate average and variance for each metric
avg_results_rf <- colMeans(results)

# Combine average and variance results into a single summary
summary_results <- data.frame(Average = avg_results_rf)

# Print the summary results
print(summary_results)


```
The error of around 15% is slightly higher than the approx 13% OOB estimate error obtained. This seems to be roughly consistent across runs so there may be some overfitting in the model.

```{R P4.4}

# Now let's compare previous models
comparison_metrics <- data.frame(
  Model = c("Random Forest", "QDA", "LDA","Weighted Logistic Regression"),
  Accuracy = c(avg_results_rf['Accuracy'], qda_accuracy, avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(avg_results_rf['Error'], qda_error, avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(avg_results_rf['Sensitivity'], qda_sensitivity, avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(avg_results_rf['Specificity'], qda_specificity, avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)
```

Comparing results so far, LDA outperforms random forest with weighted logistic regression still in the lead in terms of accuracy. Random forest surprisingly trails LDA here in all measures, so the data reacts well to linear methods and there must be clear relationships between the predictors and the response. Specificity of random forest is still not great, again the class imbalance in data may have a role here so "Y" is harder to predict correctly.

# Problem 5: SVM (points: 20/20 - graduate/undergraduate)

1. (points: 6/6) Develop SVM model of categorical outcome `response` deciding on the choice of kernel, cost, etc. that appear to yield better performance.  
2. (points: 7/7) Test model performance on multiple splits of data into training and test subsets
3. (points: 7/7) Summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of the rest of the models developed above (logistic regression, LDA, random forest).

*Be mindful of which part of data you are using to tune the hyperparameters of the model*.

```{R P5.1}

# Re-reading in the data as the prior splitting gave an odd error
data<- read.csv("final-data-train.csv")

# Split the data into training and testing sets (80/20 split)
trainIndex <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Standardize levels in both train and test data (by default is Null)
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data$response <- factor(test_data$response, levels = c("N", "Y"))

# Fit SVM Model with Radial Basis Kernel
svm_model <- svm(response ~ ., data = train_data, 
                 kernel = "radial", cost = 1, gamma = 0.1, probability = TRUE)



# Evaluate Model Performance on Test Data
svm_pred <- predict(svm_model, newdata = test_data)

# Confusion Matrix for SVM
conf_matrix_svm <- confusionMatrix(svm_pred, test_data$response)
print(conf_matrix_svm)


```

**Discussions:**

From the previous homework experience, I expect a radial kernel would be most robust here given the variety and number of predictor variables and so went ahead with it directly.

"Cost" and "Gamma" were tuned manually instead of via grid-search here with the tune function, which greatly increased runtime for me and would sometimes throw errors. Optimal cost was around 1.0 and optimal gamma was 0.1, on any other magnitude would increase error and reduce sensitivity and specificity. In fact, I found a gamma that is higher or lower 1 magnitude would drive the specificity to nearly or exactly zero, underfitting the model given the skewed classes in the training data. Makes sense given what was observed in plots in prior homework when testing gamma sensitivity.

```{R P5.2}

# 10 Splits used
num_splits <- 10

# Initialize a data frame to store results
results_svm <- data.frame(Accuracy = numeric(num_splits),
                          Error = numeric(num_splits),
                          Sensitivity = numeric(num_splits),
                          Specificity = numeric(num_splits))

# Loop through multiple splits
for (i in 1:num_splits) {
  
  data<- read.csv("final-data-train.csv")
  
  # Split the data into training and testing sets (80/20 split)
  trainIndex <- sample(1:nrow(data), size = 0.8 * nrow(data))
  train_data <- data[trainIndex, ]
  test_data <- data[-trainIndex, ]
  
  # Apply the captured levels to the training and test sets
  for (var in names(train_data)) {
    if (is.factor(train_data[[var]])) {
      
      # Relevel both training and test data to match the original factor levels
      train_data[[var]] <- factor(train_data[[var]], levels = all_factor_levels[[var]])
      test_data[[var]] <- factor(test_data[[var]], levels = all_factor_levels[[var]])
    }
  }
  
  # Standardize the response variable
  train_data$response <- factor(train_data$response, levels = c("N", "Y"))
  test_data$response <- factor(test_data$response, levels = levels(train_data$response))
  
  # Train SVM model with radial kernel
  svm_model <- svm(response ~ ., data = train_data,
                   kernel = "radial",  # RBF kernel
                   cost = 1,           # Cost parameter
                   scale = TRUE)       # Standardize the features
  
  # Evaluate Model Performance on Test Data
  svm_pred <- predict(svm_model, newdata = test_data)
  
  # Ensure predicted values are factors
  svm_pred <- factor(svm_pred, levels = levels(train_data$response))
  
  # Confusion Matrix for SVM
  conf_matrix_svm <- confusionMatrix(svm_pred, test_data$response)
  
  # Store the performance metrics
  results_svm$Accuracy[i] <- conf_matrix_svm$overall['Accuracy']
  results_svm$Error[i] <- 1 - conf_matrix_svm$overall['Accuracy']
  results_svm$Sensitivity[i] <- conf_matrix_svm$byClass['Sensitivity']
  results_svm$Specificity[i] <- conf_matrix_svm$byClass['Specificity']
}

# Calculate average for each metric
avg_results_svm <- colMeans(results_svm)

# Combine average results into a single summary
summary_results_svm <- data.frame(Average = avg_results_svm)

print(summary_results_svm)


```

```{R P5.3}

# Now let's compare all models so far
comparison_metrics <- data.frame(
  Model = c("SVM", "Random Forest", "QDA", "LDA", "Weighted Logistic Regression"),
  Accuracy = c(avg_results_svm['Accuracy'], avg_results_rf['Accuracy'], qda_accuracy, avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(avg_results_svm['Error'], avg_results_rf['Error'], qda_error,  avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(avg_results_svm['Sensitivity'], qda_sensitivity, avg_results_rf['Sensitivity'], avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(avg_results_svm['Specificity'], qda_specificity, avg_results_rf['Specificity'], avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)
```
The radial SVM results are slightly worse than LDA and Weighted Logistic Regression, but is better than random forest and qda on accuracy. Compared to LDA, its specificity is slightly better, but sensitivity is slightly worse. The SVM gave slightly more false positives (positive being "N" in our case), which is plausible again given the class imbalance in train data.

# Problem 6: predictions for test dataset  (points: 10/10 - graduate/undergraduate)

## Problem 6a: compare logistic regression, LDA, random forest and SVM model performance (points: 3/3)

Compare performance of the models developed above (logistic regression, LDA, random forest, SVM) in terms of their accuracy, error and sensitivity/specificity.  Comment on differences and similarities between them.

Depending on how detailed your comparisons were in the individual problems 1--5, you may want to add a more detailed write-up here, or just put in a summary and refer to the corresponding plots and discussions presented earlier. Copying most important/revealing plots and/or generating a dedicated new plot could be useful. 

The comparisons you were asked to perform in the previous problems were model-centric (in the form "the current model against a few others"). Here, we just expect a more leveled and "fair" summary of your earlier observations: how *all* those models compare to each otrher.

```{R P6a}

# Let's compare all previous models
comparison_metrics <- data.frame(
  Model = c("SVM", "Random Forest", "QDA", "LDA", "Weighted Logistic Regression"),
  Accuracy = c(avg_results_svm['Accuracy'], avg_results_rf['Accuracy'], qda_accuracy, avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(avg_results_svm['Error'], avg_results_rf['Error'], qda_error,  avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(avg_results_svm['Sensitivity'], qda_sensitivity, avg_results_rf['Sensitivity'], avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(avg_results_svm['Specificity'], qda_specificity, avg_results_rf['Specificity'], avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)

# Convert the data frame from wide to long format for  plotting
comparison_metrics_long <- melt(comparison_metrics, id.vars = "Model")

# Bar chart for performance metrics across all models
ggplot(comparison_metrics_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(y = "Metric Value", x = "Model", title = "Comparison of Model Performance") +
  scale_fill_manual(values = c("Accuracy" = "green", 
                               "Error" = "red", 
                               "Sensitivity" = "purple", 
                               "Specificity" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
First keep in mind for all models the positive is "N".

Comparing all models, we see a much larger variance in specificity vs sensitivity. This as mentioned prior likely has something to do with the class imbalance - Only 25% of responses are "Y" in the train data, which makes "Y" harder to predict and thus we get more false positives and a lower specificity in models compared to sensitivity, which also results in wider variance of specificity. The weighted logistic regression has the higehst specificity by far because it is weighted, otherwise logistic regression specificity would be very close to that of SVM and LDA at around 65% seen earlier in problem 2.

The accuracy ranges are not too far apart and similarly the sensitivity is usually very high at around 95%, the exception being weighed logistic regression at around 82%, which looks like a sacrifice in sensitivity for specificity.

Looking at accuracy/error overall, weighted logistic regression is best followed by LDA and SVM. Coming in last is QDA (done for extra credit only, probably could have tweak it a bit better as well).

## Problem 6b: make predictions for the **test** dataset  (points: 3/3)

Decide on the model that performs the best and use it to make predictions for the **test** ("validation") dataset provided to you (the one without the outcome labels). Upload resulting predictions in comma-separated values (CSV) format into the Canvas website.  Please check sample files with test dataset predictions for the expected format of the *.csv file: your submission must be in precisely the same format -- two and only two columns, first column - ids of the test observations ("id" column in test dataset), second - predictions as Y/N calls (not 0/1, 1/2, true/false, etc.).  The name of the second column (the one holding predictions) is what we will used in the leaderboard as the model name.

Note: I kept the code below for best model of each day I submitted predictions for before my final submission (which from first day to last is SVM, neural net, and weighted logistic regression) 

```{R P6b SVM}

# Re-read in the data
train_data<- read.csv("final-data-train.csv")
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data <- read.csv("final-data-test.csv")

# Capture factor levels from the training dataset
all_factor_levels <- list()

# Loop through each factor variable and store its levels
for (var in names(train_data)) {
  if (is.factor(train_data[[var]])) {
    all_factor_levels[[var]] <- levels(train_data[[var]])
  }
}

# Ensure the test data factors match the training levels
for (var in names(test_data)) {
  if (is.factor(test_data[[var]])) {
    # Relevel factors in the test data to match the training data levels
    test_data[[var]] <- factor(test_data[[var]], levels = all_factor_levels[[var]])
  }
}

# Train the SVM model on the training data (with scaled features)
svm_model_final <- svm(response ~ ., data = train_data, 
                       kernel = "radial", cost = 1, scale = TRUE, 
                       type = "C-classification")


# Make predictions on the test dataset
svm_pred_final <- predict(svm_model_final, newdata = test_data)
svm_pred <- ifelse(svm_pred_final == "Y", "Y", "N")

# Create a data frame with IDs and write to csv
output_data <- data.frame(id = test_data$id, `Early Bird` = svm_pred)
write.csv(output_data, "predictions-eb.csv", row.names = FALSE)


```

```{R 6b2 Neural Net}

# Re-Read in the data
train_data <- read.csv("final-data-train.csv")
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data <- read.csv("final-data-test.csv")

# Capture factor levels from the training dataset
all_factor_levels <- list()
for (var in names(train_data)) {
  if (is.factor(train_data[[var]])) {
    all_factor_levels[[var]] <- levels(train_data[[var]])
  }
}

# Ensure the test data factors match the training levels
for (var in names(test_data)) {
  if (is.factor(test_data[[var]])) {
    test_data[[var]] <- factor(test_data[[var]], levels = all_factor_levels[[var]])
  }
}

# Scale the numeric features
train_data_scaled <- train_data
test_data_scaled <- test_data
train_data_scaled[, sapply(train_data, is.numeric)] <- scale(train_data[, sapply(train_data, is.numeric)])
test_data_scaled[, sapply(test_data, is.numeric)] <- scale(test_data[, sapply(test_data, is.numeric)])

# Train neural network
nn_model_final <- nnet(response ~ ., data = train_data_scaled, size = 10, linout = FALSE, maxit = 200)

# Make predictions on the test dataset
nn_pred_final <- predict(nn_model_final, newdata = test_data_scaled, type = "class")  # Get class predictions

# Convert predictions to "Y" and "N" 
nn_pred <- ifelse(nn_pred_final == "Y", "Y", "N")

# Create a data frame with IDs and write to csv
output_data <- data.frame(id = test_data$id, `Early Bird Robust` = nn_pred)
write.csv(output_data, "predictions-ebr.csv", row.names = FALSE)

```

```{R 6b Weighted logistic regression}

# Re-Read in the data
train_data <- read.csv("final-data-train.csv")
train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data <- read.csv("final-data-test.csv")

# Capture factor levels from the training dataset
all_factor_levels <- list()
for (var in names(train_data)) {
  if (is.factor(train_data[[var]])) {
    all_factor_levels[[var]] <- levels(train_data[[var]])
  }
}

# Ensure the test data factors match the training levels
for (var in names(test_data)) {
  if (is.factor(test_data[[var]])) {
    test_data[[var]] <- factor(test_data[[var]], levels = all_factor_levels[[var]])
  }
}


# Assign class weights inversely proportional to class frequencies
class_weights <- ifelse(train_data$response == "Y",
                        1 / sum(train_data$response == "Y"),
                        1 / sum(train_data$response == "N"))

# Fit weighted logistic regression model
logistic_model_weighted <- glm(response ~ ., data = train_data, family = "binomial", weights = class_weights)

# Make predictions on the test dataset
pred_final <- predict(logistic_model_weighted, newdata = test_data, type = "response")  # Get probability predictions
pred <- ifelse(pred_final > 0.5, "Y", "N")

# Create a data frame with IDs and write to csv
output_data <- data.frame(id = test_data$id, `Early Bird Weighty` = pred)
write.csv(output_data, "predictions-ebw.csv", row.names = FALSE)

```

## Problem 6c: get better than coin flip by 10%  (points: 4/4)

This is not really a problem *per se* but rather a criterion that we will go by when assessing quality of your predictions for the test dataset.  You get these four points if your predictions for **test** dataset are better than those obtained from a fair coin flip (already shown in leaderboard and provided among the examples of the file format for predictions upload) by at least 10% on **all** four metrics shown in the leaderboard (accuracy, sensitivity, specificity and precision).  But then, predictions by the coin flip should not be very difficult to improve upon.  

Think I exceeded all the metrics given the "Early Bird Robust" topped the leaderboard on Dec 6.

# Extra points problem: neural network model (points: 10/10 - graduate/undergraduate)

Experiment with fitting neural network models of categorical outcome `response` for this data and evaluate their performance on different splits of the data into training and test. Compare model performance to that for the rest of classifiers developed above.

```{R Extra Point P7}

# Re-reading and splitting data
set.seed(42)
data <- read.csv("final-data-train.csv")

trainIndex <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

train_data$response <- factor(train_data$response, levels = c("N", "Y"))
test_data$response <- factor(test_data$response, levels = c("N", "Y"))

# I needed to scale the numeric data or else it would underfit and predict all N,
# Given the imbalance in the training data
train_data_scaled <- train_data
test_data_scaled <- test_data

train_data_scaled[, sapply(train_data, is.numeric)] <- scale(train_data[, sapply(train_data, is.numeric)])
test_data_scaled[, sapply(test_data, is.numeric)] <- scale(test_data[, sapply(test_data, is.numeric)])

# Fit neural network model with more hidden units and epochs
# Larger size and iterations isn't necessarily better here, I tested different hyperparameters
# 20 Iterations was enough and reduced runtime as well
nn_model <- nnet(response ~ ., data = train_data_scaled, size = 5, linout = FALSE, maxit = 20)

# Predict the responses on the test data
nn_pred <- predict(nn_model, newdata = test_data_scaled, type = "raw")  # Get raw probabilities

# Convert numeric predictions to factor labels (N = 0, Y = 1)
nn_pred <- ifelse(nn_pred > 0.5, "Y", "N")
nn_pred <- factor(nn_pred, levels = c("N", "Y"))

# Confusion Matrix for Neural Network
conf_matrix_nn <- confusionMatrix(nn_pred, test_data$response)
print(conf_matrix_nn)

# Check the model summary
summary(nn_model)

```
I tweaked the hyperparameters to optimize performance and runtime at around size=5 and maxiter=20.

```{R P7.2}

# Re-reading and splitting data later below
set.seed(42)  
data <- read.csv("final-data-train.csv")

# Iterations for sampling
num_iterations <- 10

# Initialize a data frame to store the results
results <- data.frame(
  Iteration = 1:num_iterations,
  Accuracy = numeric(num_iterations),
  Sensitivity = numeric(num_iterations),
  Specificity = numeric(num_iterations),
  Error = numeric(num_iterations)
)

# Loop over each iteration
for (i in 1:num_iterations) {
  
  # Randomly sample 80% of the data for training (20% for testing)
  trainIndex <- sample(1:nrow(data), size = 0.8 * nrow(data))
  train_data <- data[trainIndex, ]
  test_data <- data[-trainIndex, ]
  
  # Standardize levels in both train and test data
  train_data$response <- factor(train_data$response, levels = c("N", "Y"))
  test_data$response <- factor(test_data$response, levels = c("N", "Y"))
  
  # Scale the numeric data
  train_data_scaled <- train_data
  test_data_scaled <- test_data
  
  train_data_scaled[, sapply(train_data, is.numeric)] <- scale(train_data[, sapply(train_data, is.numeric)])
  test_data_scaled[, sapply(test_data, is.numeric)] <- scale(test_data[, sapply(test_data, is.numeric)])
  
  # Fit neural network  
  nn_model <- nnet(response ~ ., data = train_data_scaled, size = 5, linout = FALSE, maxit = 100)
  
  # Predict the responses on the test data
  nn_pred <- predict(nn_model, newdata = test_data_scaled, type = "raw")  # Get raw probabilities
  
  # Convert numeric predictions to factor labels (N = 0, Y = 1)
  nn_pred <- ifelse(nn_pred > 0.5, "Y", "N")
  nn_pred <- factor(nn_pred, levels = c("N", "Y"))
  
  # Confusion Matrix for Neural Network
  conf_matrix_nn <- confusionMatrix(nn_pred, test_data$response)
  
  # Store the performance metrics for the current iteration
  results$Accuracy[i] <- conf_matrix_nn$overall['Accuracy']
  results$Sensitivity[i] <- conf_matrix_nn$byClass['Sensitivity']
  results$Specificity[i] <- conf_matrix_nn$byClass['Specificity']
  results$Error[i] <- 1 - conf_matrix_nn$overall['Accuracy']
}

# Print the results
print(results)
avg_results <- colMeans(results[, -1])
print("\nAverage performance across all iterations:")
print(avg_results)


```
We do see some significant enough variance across runs, particularly in specificity. 

```{R P7.3}
# Compare vs previous models
comparison_metrics <- data.frame(
  Model = c("Neural Net","SVM", "Random Forest", "QDA", "LDA", "Weighted Logistic Regression"),
  Accuracy = c(avg_results['Accuracy'], avg_results_svm['Accuracy'], avg_results_rf['Accuracy'], qda_accuracy, avg_results_lda['Accuracy'], mean(weighted_logistic_accuracy)),
  Error = c(avg_results['Error'], avg_results_svm['Error'], avg_results_rf['Error'], qda_error,  avg_results_lda['Error'], mean(weighted_logistic_error)),
  Sensitivity = c(avg_results['Sensitivity'], avg_results_svm['Sensitivity'], qda_sensitivity, avg_results_rf['Sensitivity'], avg_results_lda['Sensitivity'], mean(weighted_logistic_sensitivity)),
  Specificity = c(avg_results['Specificity'], avg_results_svm['Specificity'], qda_specificity, avg_results_rf['Specificity'], avg_results_lda['Specificity'], mean(weighted_logistic_specificity))
)

print(comparison_metrics)

# Convert the data frame from wide to long format for  plotting
comparison_metrics_long <- melt(comparison_metrics, id.vars = "Model")

# Bar chart for performance metrics across all models
ggplot(comparison_metrics_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(y = "Metric Value", x = "Model", title = "Comparison of Model Performance") +
  scale_fill_manual(values = c("Accuracy" = "green", 
                               "Error" = "red", 
                               "Sensitivity" = "purple", 
                               "Specificity" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Comparison:

We now see Neural net shows the best overall metrics with an accuracy of c.89.8%, while sensitivity may not be the highest among all models, it is still around 94% which is close to the highest of around 96%. Specificity however is approx.77% which is better than LDA at approx 74% and much better than the other specificities in the 50%-70% range.

# An afterword on the computational demands of the final exam

During previous offerings of this course there were always several posts on discussion board regarding how long it takes to fit various classifiers to the final exam dataset; hence this added note here.

We most definitely do *not* expect you to *have* to buy capacity from AWS to complete this assignment. You certainly can if you want to, but this course is not about that and this dataset is really not *that* big to require it. Something reasonable/useful can be accomplished for this data with middle of the road hardware. For instance, knitting of the entire official solution for the final exam on 8Gb RAM machine with two i5-7200u cores takes about an hour using single-threaded R/Rstudio and this includes both extra points problems as well as various assessments of the performance of different models as function of data size and so on.

But that also means, of course, that if you choose to *rerun* the whole dataset multiple times while just developing and debugging your code, you might indeed find yourself in a bind. A much more efficient and intelligent development methods exist than throwing money and power of dozens of GPUs onto a problem. Use smaller subsets of data to better understand them and to develop and debug your code. Use full dataset only for the final clean runs. And the chances are you will be fine. 

Even your final solution should not take hours and hours to compile. If your final clean run of presumably fully debugged and otherwise working code takes forever, then the chances are (1) you are attempting to do too much, or (2) something is implemented inefficiently, or just plain incorrectly. It is impossible for us to comment on this until we see the code when we grade it. As just one possible hint -  think about the loops (for instance the densities of your tuning grids - are you sure you want to go wild and tune on a grid of 50 cost values, times 30 $gamma$ values? maybe you want to find an approximate optimum first, on a much coarser-grained grid?). 

In general, it is often very prudent to "start small" -- fit your model on a random subset of data small enough for the model fitting call to return immediately, check how model performance (both in terms of error and time it takes to compute) scales with the size of the data you are training it on (as you increase it in size, say, two-fold several times), for tuning start with very coarse grid of parameter values and given those results decide what it right for you, etc.

Lastly, making the decision about what is right for the problem at hand, how much is enough, etc. is inherent in this line of work. If you choose to conduct model tuning on a subset of the data - especially if you have some assessment of how the choice of tuning parameter and test error is affected by the size of training dataset - it could be a very wise choice.  If it is more efficient for you to knit each problem separately, by all means feel free to do that - just remember to submit each .Rmd and HTML file that comprises your entire solution. On that note, if you end up using any of the unorthodox setups for your calculations (e.g. AWS, parallel processing, multiple machines, etc. - none of which are essential for solving it correctly) please be sure that when we grade we have every relevant piece of code available - we won't be necessarily able to grade your work if we are not clear about how the results were obtained.

In the end, the final exam asks you to assess performance of several classification technologies on a new dataset, decide on which classifier is the best and use it to make predictions for the test data. It is very much up to you how exactly you want to go about it.  There could be many versions of correct and informative solution for that (as there could be just as many if not more that are completely wrong).

As always, best of luck - we are practically done here!
