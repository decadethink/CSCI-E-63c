---
title: "CSCI E-63C: Week 4 Problem Set"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# Preface

This week problem set is focused on using resampling (specifically, bootstrap) to estimate and compare training and test error of linear models with progressively increasing number of predictor variables included.

For this exercise we will use all the "originally measured" predictor variables, as well as their 2nd degree powers and pairwise products (i.e. we also include interaction terms). 

The goal is to advance your familiarity with fitting multiple regression models, to develop hands-on experience with the use of resampling and to observe first-hand the discrepancy in the trending of training and test error with the increase in model complexity.

For this assignment you will continue working with the fundraising dataset that we already familiarized ourselves with during the previous two weeks.  However, the examples shown in this preface will use a *simulated* dataset to illustrate the main steps required for completing the problem set (so it will be up to you to adapt the code as needed).

We start with developing a function in R that would provide a convenience shortcut for generating a simulated dataset with the specified numbers of observations and of predictor variables. The data are simulated as follows:

* we "measure" a certain number of variables (specified by parameter `N_X_vars`), and we make `N_obs` observations (i.e. the latter is the number of rows in the table with simulated data).
* we then add all second degrees and pairwise products of those "original" variables to the dataset as additional columns (in other words, if we simulate variables $X_1,...,X_3$, then we will also add to the data the columns filled with values for $X_1^2,...,X_3^2, X_1\cdot X_2, X_1 \cdot X_3, X_2 \cdot X_3$).
* we simulate the outcome variable $Y$ as a mean of *some* of the predictor variables. The parameter `Y_dep_idx` is a 
vector of indexes of the predictor variables the outcome indeed depends on. For instance, if `Y_dep_idx=c(1,3)`, then the outcome will be simulated as $Y=(X_1+X_3)/2$ while all other "predictor" variables will have no effect on the outcome. Note that here we specify the desired dependence on any subset of *all* the predictor variables available in the data (including those higher powers and interaction terms if one wants to).  
* Finally, we add normal noise of specified width (`SDerr`) to the outcome so that we have a proper statistical problem with noise present.

The function will return the dataset containing the outcome and all the predictor variables, computed as described above, as a dataframe.

Clearly, all those assumptions we build into our simulation provide only an example of how some data *might* look. Admittedly, it is a gross oversimplification of any real-life dataset typically encountered in the wild. However, it will suffice for our purposes (seeing through resampling and observing the divergence of training and test errors as the model complexity increases):

```{r simuLinQuadDat}
simulateLinQuadData <- function(N_obs=100, N_X_vars=5, Y_dep_idx=1:2, SDerr=0.5) {
  # Simulate N_obs observations of N_X_vars normal variables, 
  # save the result as a matrix (rows = observations, columns = variables):
  xTmp <- matrix(rnorm(N_obs*N_X_vars),ncol=N_X_vars)
  colnames(xTmp) <- paste0("X",1:N_X_vars)
  # Now generate additional predictor variables as all pairwise products of the
  # variables X1, ..., Xn (including by itself), X1*X1, X1*X2, X1*X3, ..., Xn*Xn:
  x2Tmp <- NULL  # matrix of the higher order variables we want to append
  tmpColnames <- NULL  # vector of variable names, we will build it as we go
  # for each original variable:
  for ( i in 1:ncol(xTmp) ) {
    # multiply it by itself and by all other (original) variables, one by one,
    # but excluding pairwise combinations that were already generated: 
    for ( j in i:ncol(xTmp) ) {
      x2Tmp <- cbind(x2Tmp,xTmp[,i]*xTmp[,j])
      # append the name for the column we just generated:
      tmpColnames <- c(tmpColnames,paste0("X",i,"X",j))
    }
  }
  # now assign the column names:
  colnames(x2Tmp) <- tmpColnames
  # ...and bind the "original" variables and higher-order predictors
  # built from them together:
  Xdat <- cbind(xTmp,x2Tmp)
  
  # create outcome as an average (i.e. equally-weighted sum) of the 
  # specified variables only, plus some noise:
  y <- rowMeans(Xdat[,Y_dep_idx])+rnorm(N_obs,sd=SDerr)
  
  data.frame(Y=y,Xdat)
}
```

Once again, for this problem set you will have to work with fund-raising data, so that you 
*won't have* to simulate anything -- we are doing it here, in the preface, only to
put together an example.  However, the problem set will ask you to examine the
effects of including 2nd powers and pairwise products of the continuous attributes into the model, so  you might want to incorporate some aspects of the code examples we show into your work.

Now, let's start with simulating a dataset using the default parameters (100 observations; 5 independently drawn normally distributed variables, plus all their 2nd powers and pairwise products; and the outcome actually depends *only* on $X_1$ and $X_2$ (linearly)). We are going to take a quick look at the simulated dataset and fit a couple of linear models right away. 
Note  the effect of specifying `fig.width` and `fig.height` in the RMD file inside the code chunk attribute section (curly brace-delimited clause right after the `\`\`\``): it allows specifying the dimensions (in the HTML output document) for the figure generated in that code chunk:

```{r exampleSimulDat, fig.width=12,fig.height=12}
simDat <- simulateLinQuadData()
class(simDat)
dim(simDat)
head(simDat)
pairs(simDat[,1:5], pch=19, cex=0.6)
```

For $N_X_var=k$ linear variables our function returns $p=1+k+k*(k+1)/2$ columns 
(outcome, linear terms, plus all unique pairwise quadratic combinations). Since, by default, the outcome is the average of the first two attributes (with added noise), those attributes do show correlation with the $Y$ in the pairwise scatterplots we just generated, while the other predictors look uncorrelated to the outcome, as they should.

For the purposes of model fitting, the terms we want to include in the model
can be explicitly provided by the formula, as in:

```{r simDatLmExplEq}
lm(Y~X1+X2+X1X1+X1X2+X2X2,simDat)
```

Alternatively, one can pass to `lm()` a dataframe that includes only the desired
subset of columns, and then use a special variant of formula, `Y ~ .` that means 
"fit $Y$ against every other variable (column) *present in the data*":

```{r simDatLmSubsetAll}
# same result as above: here we subset the data to the columns we want to use
# and then use "fit against everything available" formula:
lm(Y~.,data=simDat[,c("Y","X1","X2","X1X1","X1X2","X2X2")])
```

Of course it does not matter how we choose to select the subset of data: we could
use numeric indices into `data.frame` columns instead, or the `select()` verb
from `dplyr` library, etc:

```{r simDatLmSubsetIdx}
lm(Y~.,data=simDat[,c(1:3,7,8,12)])
```

Finally, one can of course dynamically build the formula itself:

```{r asformulaexample}
f_str <- paste0("Y~",paste(colnames(simDat)[c(2:3,7,8,12)],collapse="+"))
# just to see what these 'paste' commands produce:
print(f_str)
# now use that dynamically built string as a formula:
lm(as.formula(f_str),data=simDat)
```

Note that in the example above we are building (dynamically) a formula that refers
to a subset of variables in the dataframe, while the latter already has those
additional predictors (higher order terms) precomputed. Alternatively, one could use
just the data with the original, measured variables, and ask all the additional higher
order terms to be computed dynamically while the model is being fitted (i.e. 
we could just use the dataframe with variables $Y, X1, X2, ..., X5$, but specify
*formulas* as, for instance, `Y~X1+X2+X3+I(X1^2)+I(X2^2)+X1:X2`, or `Y~X1+X2+X3+X1*X2` - those two examples happen to be *exactly* the same model!). In the latter case we might be saving some memory, but building all those more complicated formulas dynamically would be more difficult, hence we chose to precompute all the predictors once (including higher powers), and then just loop over subsets of those precomputed variables. With a dataset that small, it should not pose any issues.

There is no single right answer as to which of all those alternative ways one should choose if the goal is examining multiple models. Use any technique that seems to make the most sense in the situation at hand. However, one might argue that *explicit* inclusion of model terms in the formula is more likely to be suitable for interactive model fitting session, or for presenting some specific, hand picked results in your reports. 

But if you are planning on running a loop over many different models (or even you have just a handful of models to compare but they all include large numbers of variables, so the explicit formulas become impractical anyway), you can specify models dynamically in your code by either

(1) building a string for the model formula automatically (using `paste` with suitable values for `collapse`, e.g. `"+"`, `"*"` and/or `":"`, called on a proper subset of data frame column names), then convert to a formula object with `as.formula()` command; or
(2) dynamically select and pass different subsets of data and simply use the "Y~." type of formula as it was demonstrated above.

Let's now create a dataset with $n=200$ observations and $k=6$ linear predictors (and corresponding quadratic terms) where outcome is the average of the first three linear terms plus some noise. Then let us fit linear models starting with one predictor term all the way to all linear and quadratic terms pre-computed and included in our dataframe.
Finally, we are going to plot the resulting (average) error of each model:

```{r simul200, fig.width=6,fig.height=6}
simDat <- simulateLinQuadData(N_obs=200, N_X_vars=6, Y_dep_idx=1:3, SDerr=1)
df2plot <- NULL
for ( i in 2:ncol(simDat) ) {
  lmTmp <- lm(Y~.,simDat[,1:i]) # fit Y against all variables 1 through i
  errTmp <- sqrt(mean((simDat[,"Y"]-predict(lmTmp))^2)) # compute mean square error
  # save the current number of variables used and the model mean square error:
  df2plot <- rbind(df2plot,data.frame(nvars=i-1,err=errTmp))
}
plot(df2plot,
     xlab="Number of variables",ylab="Regression error",
     main=paste(nrow(simDat),"observations"),
     ylim=c(min(df2plot[,"err"]),sqrt(mean((simDat[,"Y"]-mean(simDat[,"Y"]))^2))))
abline(h=sqrt(mean((simDat[,"Y"]-mean(simDat[,"Y"]))^2)),lty=2)
```

The horizontal dashes in the plot indicate the error of the trivial model predicting 
the outcome without using any predictors (i.e. simply predicting mean $Y$). As one would expect, inclusion of the first three predictors (average of which plus noise *is* the outcome) results in the most dramatic decrease in the mean squared error between observed and predicted outcome (that is training error, of course -- because it is calculated on the same dataset that the model was fit to). But note that the initial quick drop is followed by the gradual decrease in the error as more model terms are incorporated.  

Note that here we pretend to know which predictors are the most important and have to be included first and in which order the rest of them have to be added.  More disciplined approaches would involve ordering predictors by their corresponding model improvement at each step of variable selection and will be explored next week. Here we use this shortcut for the purposes of simplicity to allow us to focus on resampling and the difference between training and test errors.

Two more caveats that need to be discussed here concern with the notion of 
degrees of freedom. First, once again for simplicity, the training error as calculated above is different from `sigma` slot in the output of `summary()` by $\sqrt{n/(n-p-1)}$ where $n$ is the number of observations and $p$ -- number of the variables included in the model (aka degrees of freedom used up by the model).  For more details, see for instance, corresponding section in [LHSP](http://www.jerrydallal.com/LHSP/dof.htm) -- that is a great source of all kinds of practical statistical wisdom.  

Second, as the number of variables included in the model approaches about 10-20, for a given number of observations ($n=200$) it starts to exceed maximal recommended ratio of the number of observations to the number of predictors included in the model, which is also about 10-20, at least for the kind of signal/noise ratios typically encountered in biomedical and social sciences. In other words, fitting model with 27 variables on 200 observations is usually a bad idea, but we will see below that the discrepancy between training and test error for our examples kicks in way before that.

Back to the demo -- the plot above demonstrates that the training error continues to decrease as the model complexity increases. How the training and test errors would look like if model is trained on a bootstrap of the data and tested on the subset of the data not included in the bootstrap?  First, again, let's write a function evaluating inclusion of one to all predictors, each on a number of bootstrap resamplings. For the purposes of clarity and simplicity here we disregard existing bootstrap facilities that are available in R in packages such as `boot` and implement a simple bootstrap resampling directly:

```{r bootTrainTestFun}
bootTrainTestErrOneAllVars <- function(inpDat,nBoot=100) {
  # matrices to store bootstrap training and test errors; columns will correspond to
  # models with different numbers of variables included (from 1 through all of them) and 
  # rows will correspond to bootstrap iterations; for instance, the errors of models with one and two variables 
  # (Y~X1 and Y~X1+X2) will be found in columns 1 and 2, respectively, and those columns will
  # store nBoot values:
  errTrain <- matrix(NA,nrow=nBoot,ncol=ncol(inpDat)-1)
  errTest <- matrix(NA,nrow=nBoot,ncol=ncol(inpDat)-1)
  # vector to store the training error of each model
  # fitted on all observations:
  allTrainErr <- numeric()
  # this loop tries different models: with 1, 2, 3, ... predictor variables
  # (note that first predictor is the second column in
  # the input data - the first column is the outcome "Y"):
  for ( iTmp in 2:ncol(inpDat) ) {
    # fit model that inludes iTmp-1 first variables (in the order they occur
    # in the columns of inpDat):
    lmTmp <- lm(Y~.,inpDat[,1:iTmp])
    
    # (use summary(lmTmp)$sigma if you want correction for the degrees of freedom)
    # training error of the current model (iTmp-1 predictors) fitted on all data: 
    allTrainErr[iTmp-1] <- sqrt(mean((inpDat[,"Y"]-predict(lmTmp))^2))
    
    # Now let's cross-validate the current model using bootstrap.
    # Perform nBoot intependent bootstrapping resamplings to accumulate the statistics:
    for ( iBoot in 1:nBoot ) {
      # in each resampling iteration, generate a new bootstrapped sample
      # (replace=TRUE is critical for bootstrap to work correctly!):
      tmpBootIdx <- sample(nrow(inpDat),nrow(inpDat),replace=TRUE)
      # now tmpBootIdx are the indexes of observations in the original data that were 
      # randomly selected (in this particular bootstrap iteration) to be placed into the training set.
      # Now fit the current model on the bootstrap sample:
      lmTmpBoot <- lm(Y~.,inpDat[tmpBootIdx,1:iTmp])
      # calculate the MSE for the current bootstrap iteration
      # (or use summary(lmTmpBoot)$sigma for degrees of freedom correction):
      errTrain[iBoot,iTmp-1] <- sqrt(mean((inpDat[tmpBootIdx,"Y"]-predict(lmTmpBoot))^2))
      # test error is calculated on the observations
      # =not= in the bootstrap sample - thus "-tmpBootIdx"
      errTest[iBoot,iTmp-1] <- sqrt(mean((inpDat[-tmpBootIdx,"Y"]-
                                            predict(lmTmpBoot,newdata=inpDat[-tmpBootIdx,1:iTmp]))^2))
    }
  }
  # return results (bootstrap training errors, bootstrap test errors, and training errors
  # claculated on the full dataset) as different slots in the list:
  list(bootTrain=errTrain,bootTest=errTest,allTrain=allTrainErr)
}
```

Let's calculate training and test bootstrap errors (as well as training error on all observations) on the dataset we have already generated previously and plot them as function of the number of variables in the model:

```{r bootErr200,fig.width=6,fig.height=6}
# wrapper for plotting:
# takes the result returned by our bootstrapping functions. colMeans() will calculate the *average*
# train and test errors (averaged across rows, i.e. across multiple bootstrap iterations):
plotBootRegrErrRes <- function(inpRes,inpPchClr=c(1,2,4),...) {
  matplot(1:length(inpRes$allTrain),cbind(inpRes$allTrain, 
                                          colMeans(inpRes$bootTrain),
                                          colMeans(inpRes$bootTest)
                                          ),
          pch=inpPchClr,col=inpPchClr,lty=1,type="b",xlab="Number of predictors",ylab="Regression error",...)
  legend("topright",c("train all","train boot","test boot"),
         col=inpPchClr,text.col=inpPchClr,pch=inpPchClr,lty=1)
}
bootErrRes <- bootTrainTestErrOneAllVars(simDat,30)
plotBootRegrErrRes(bootErrRes,main="200 observations",ylim=c(min(colMeans(bootErrRes$bootTrain)),sqrt(mean((mean(simDat[,"Y"])-simDat[,"Y"])^2))))
abline(h=sqrt(mean((mean(simDat[,"Y"])-simDat[,"Y"])^2)),lty=2)
```

Notice how test error starts to increase once all variables truly associated with the outcome has been already included in the model, while training errors continue to decrease reflecting overfit (and increasing contribution of the variance term to the model error).

Lastly, let's repeat this exercise for two larger numbers of observations simulated under the same conditions:

```{r simulThreeSz,fig.width=12,fig.height=4}
old.par <- par(mfrow=c(1,3))
for ( tmpNobs in c(200,500,1000) ) {
  simDat <- simulateLinQuadData(N_obs=tmpNobs, N_X_vars=6, Y_dep_idx=1:3, SDerr=1)
  bootErrRes <- bootTrainTestErrOneAllVars(simDat,30)
  plotBootRegrErrRes(bootErrRes,main=paste(tmpNobs,"observations"),ylim=c(min(colMeans(bootErrRes$bootTrain)),sqrt(mean((mean(simDat[,"Y"])-simDat[,"Y"])^2))))
  abline(h=sqrt(mean((mean(simDat[,"Y"])-simDat[,"Y"])^2)),lty=2)
}
par(old.par)
```

Notice how  the increase in test error with the number of predictors becomes less pronounced for larger numbers of observations.

To conclude, the examples above present code and analyses that are very close to what you will need to complete this week problem set.  Please feel free to start with those examples and modify them as necessary.  And, as always, do ask questions if anything seems unclear.


# Problem: estimating multiple regression error rate by resampling

This week problem set closely follows what was just explained in the preface, except that instead of using simulated dataset, you are expected to use the fundraising dataset that you have already worked with during the previous weeks. Note that the dataset  now also has predicted values (column `predcontr`) for the outcome as described below. It is available on the course website in canvas (file `fund-raising-with-predictions.csv` on the Canvas page for this lecture).

The first column -- `contrib` -- is the outcome we are trying to predict.  The last column -- `predcontr` -- represents *predictions* for that outcome variable made by one of the simpler models submitted by a participant in the data mining competition that used this fund raising dataset back in the 90's.  We will use the attribute `predcontr` only as a *reference* (we will compare it to our own models' predictions).  It has to be excluded from model building (it would not be prudent to try
predicting the outcome using predictions already made for that same outcome by someone else - not in the settings of this assignment at least!). 

The column before last -- `gender` -- is a categorical attribute that we will also omit for the purposes of this week problem set in the interests of simplicity.  In the end you should be working with a dataset with twelve continuous attributes -- one outcome, `contrib`, and eleven predictors (`gapmos`,	`promocontr`,	`mincontrib`,	`ncontrib`,	`maxcontrib`,	`lastcontr`,	`avecontr`,	`mailord`,	`mindate`,	`maxdate`	and `age`).  Because of distributional properties of multiple attributes in this dataset, you are better off with log-transforming *both* predictors and the outcome.  Because several values in this dataset are zeroes and in order to avoid dealing with NaNs, just add "1" before the log-transform to all values in this dataset (e.g. `myDat <- log(myDat+1)`).

## Sub-problem 1: prepare the dataset (points: 5/10 – graduate/undergraduate)

Read in the dataset, drop categorical attribute (`gender`) and predictions by the competition participants (`predcontr`), log-transform predictors and outcome as described above, rearrange columns in the dataset in the decreasing order of absolute values of their correlation with the outcome (`contrib`).  So that in the resulting data table the outcome (`contrib`) is the first column, the next one is the predictor that is the most (positively or negatively) correlated with it, and so on.  You may find it convenient to use R function `order` for that.

```{R P1.1}
# Read dataset
myDat <- read.csv("fund-raising-with-predictions.csv")

# Drop the categorical attributes
myDat <- myDat[, !names(myDat) %in% c("gender", "predcontr")]

# Log-transform the predictors and the outcome 
myDat <- log(myDat + 1) # (adding 1 to avoid NaN)

# Calculate correlations matrix
correlations <- abs(cor(myDat))
print(correlations)

# Rearrange columns based on correlation with outcome
correlation_order <- order(correlations[, "contrib"], decreasing = TRUE)

# Reorder the dataset columns
myDat <- myDat[, c("contrib", names(myDat)[correlation_order][names(myDat)[correlation_order] != "contrib"])]

# View the prepared dataset
head(myDat)
```


## Sub-problem 2: add quadratic terms to the dataset (points: 10/10 – graduate/undergraduate)

Use the code presented in the preface as a template to develop your own procedure for adding to the fund raising dataset containing outcome (`contrib`) and all continuous predictors (`gapmos` through `age`) all pairwise products of continuous predictors (e.g. `gapmos` x `gapmos`, `gapmos` x `promocontr`, ..., `age` x `age`).  The data used here has to be the one from fund raising dataset, _not_ simulated from normal distribution as shown in the preface.  In the end your dataset should have 78 columns: `contrib`, 11 predictors and 11*12/2=66 of their pairwise combinations.

```{R P1.2}
# List of all continuous predictors (excluding the outcome)
predictors <- names(myDat)[2:ncol(myDat)]

# Create a function to generate quadratic terms
generate_quadratic_terms <- function(data, predictors) {
  for (i in 1:length(predictors)) {
    for (j in i:length(predictors)) {
      # Create pairwise product terms and add them to the dataset
      new_col_name <- paste(predictors[i], predictors[j], sep = "X")
      data[[new_col_name]] <- data[[predictors[i]]] * data[[predictors[j]]]
    }
  }
  return(data)
}

# Apply the function to myDat to generate the quadratic terms
myDat_quadratic <- generate_quadratic_terms(myDat, predictors)

# View the updated dataset with quadratic terms
head(myDat_quadratic)

```

## Sub-problem 3: fit multiple regression models on the entire dataset (points: 10/10 – graduate/undergraduate)

As illustrated in the preface above, starting from the first, most correlated with `contrib`, predictor, fit linear models with one, two, ..., all 77 linear and quadratic terms on the entire dataset and calculate resulting (training) error for each of the models. Plot error as a function of the number of predictors in the model (similar to the plot in the preface that shows just the training error on the entire dataset).  Also, indicate in the same plot the error from predicting all outcomes to be just the average outcome on the entire dataset as shown in the preface.  Because the underlying data is different, the change of the regression error with the number of attributes included in the model in the plot that you obtain here for fund raising dataset will be different from that shown in the preface.  Please comment on this difference.

```{R P1.3}

# Initialize vectors to store the number of predictors and corresponding training errors
num_predictors <- 1:(ncol(myDat_quadratic) - 1)
train_errors <- numeric(length(num_predictors))

# Loop over the number of predictors to fit models incrementally
for (i in num_predictors) {
  # Fit a linear model using the first 'i' predictors
  model <- lm(contrib ~ ., data = myDat_quadratic[, 1:(i + 1)])
  
  # Calculate training error
  train_errors[i] <- sqrt(mean((myDat_quadratic$contrib - predict(model, myDat_quadratic[, 1:(i + 1)]))^2))
}

# Calculate the error when predicting all outcomes as the average
avg_error <- sqrt(mean((myDat_quadratic$contrib - mean(myDat_quadratic$contrib))^2))

# Plot training error as a function of the number of predictors
plot(num_predictors, train_errors, type = "b", col = 'blue', pch = 19,
     xlab = "Number of Predictors", ylab = "Training Error",
     main = "Training Error vs. Number of Predictors",
     ylim = c(min(train_errors), max(avg_error, max(train_errors))))
abline(h = avg_error, col = "red", lty = 2)

# Add a legend to indicate the error from predicting the average
legend("topright", legend = c("Training Error", "Average Prediction Error"),
       col = c("blue", "red"), lty = c(1, 2), pch = c(19, NA))

```
**Comments:**

The preface plot shows a more gradual decline in the regression error as the number of variables increases, while in the case above, the error seems to decrease more quickly initially and then stabilizes and flatlines. This suggests that in the preface example, adding additional variables helps more incrementally, whereas in the fundraising contribution dataset, the initial variables already capture most of the variance in the target variable (As we did sort the columns by correlation).

## Sub-problem 4: develop function performing bootstrap on fund raising dataset (points: 5/10 – graduate/undergraduate)

Modify function `bootTrainTestErrOneAllVars` defined in the preface to perform similar kind of analysis on the fund raising dataset.  Alternatively, you can determine what modifications are necessary to the fund raising dataset, so that it can be used as input to `bootTrainTestErrOneAllVars`.

```{R P1.4}
bootTrainTestErrFundraising <- function(inpDat, responseVar="contrib", nBoot=100) {
  # Make sure response variable is in the first column and predictors follow
  inpDat <- inpDat[, c(responseVar, setdiff(colnames(inpDat), responseVar))]
  
  # Matrices to store bootstrap training and test errors
  errTrain <- matrix(NA, nrow=nBoot, ncol=ncol(inpDat) - 1)
  errTest <- matrix(NA, nrow=nBoot, ncol=ncol(inpDat) - 1)
  
  # Vector to store training error of each model fitted on all observations
  allTrainErr <- numeric()
  
  # Loop over models with increasing number of predictors
  for (iTmp in 2:ncol(inpDat)) {
    # Fit model with iTmp - 1 predictors
    lmTmp <- lm(as.formula(paste(responseVar, "~ .")), inpDat[, 1:iTmp])
    
    # Training error on the full dataset
    allTrainErr[iTmp - 1] <- sqrt(mean((inpDat[[responseVar]] - predict(lmTmp))^2))
    
    # Bootstrap resampling
    for (iBoot in 1:nBoot) {
      # Bootstrap sample
      tmpBootIdx <- sample(nrow(inpDat), nrow(inpDat), replace=TRUE)
      lmTmpBoot <- lm(as.formula(paste(responseVar, "~ .")), inpDat[tmpBootIdx, 1:iTmp])
      
      # Training error on bootstrap sample
      errTrain[iBoot, iTmp - 1] <- sqrt(mean((inpDat[tmpBootIdx, responseVar] - predict(lmTmpBoot))^2))
      
      # Test error on the observations not in the bootstrap sample
      errTest[iBoot, iTmp - 1] <- sqrt(mean((inpDat[-tmpBootIdx, responseVar] -
                                             predict(lmTmpBoot, newdata=inpDat[-tmpBootIdx, 1:iTmp]))^2))
    }
  }
  
  # Return training and test errors
  list(bootTrain=errTrain, bootTest=errTest, allTrain=allTrainErr)
}

```


## Sub-problem 5: use bootstrap to estimate training and test error on fund raising dataset (points: 20/20 – graduate/undergraduate)

Use function developed above to estimate training and test error in modeling `contrib` for the fund raising dataset.  Plot and discuss the results.  Compare your model errors obtained over the range of model complexity (in other words over the range of
numbers of variables included) to the error of the model built by the competition participants (we know their predictions, so their residuals are differences between `contrib` and `predcontr` in the original full dataset; since we used the log-transform before proceeding with our own modeling, this needs accounted for -- make sure that you either calculate
the error of the competition participants on the same log scale or recalculate your own errors back to the original scale before the comparison!)


```{R P1.5}
# Assuming you have your bootstrapping results in 'result' from the bootTrainTestErrFundraising
result <- bootTrainTestErrFundraising(myDat_quadratic, responseVar="contrib", nBoot=100)

# Now using plotBootRegrErrRes to plot
plotBootRegrErrRes(result, main="Fundraising Dataset Bootstrap Errors",
                   ylim=c(min(c(colMeans(result$bootTrain), colMeans(result$bootTest))),
                          max(c(colMeans(result$bootTrain), colMeans(result$bootTest)))))

```
```{R P1.5.2}
# Compare to competition participant errors
origDat <- read.csv("fund-raising-with-predictions.csv")
competition_errors <- (log(origDat$contrib) - log(origDat$predcontr))^2
competition_error <- sqrt(mean(competition_errors))

# Output the comparison of model errors
cat("Average Training Error:", mean(result$bootTrain), "\n")
cat("Average Test Error:", mean(result$bootTest), "\n")
cat("Competition Model Error:", competition_error, "\n")

res <- bootTrainTestErrFundraising(myDat, responseVar="contrib", nBoot=100)

plotBootRegrErrRes(res, main="Fundraising Dataset Bootstrap Errors",
                   ylim=c(min(c(colMeans(result$bootTrain), colMeans(result$bootTest))),
                          competition_error))
abline(h=competition_error, col="green", lty=2)  # Competition model error 

legend("topright", legend=c("Training Error", "Test Error", "Train All","Competition Error"), col=c("blue", "red", "black","green"), lty=1:2)

```

**Discussion:**

Similar to the preface example, the training error decreases as more predictors are added and the bootstrapping line is similar at slightly lower error, which indicates overfitting.

This contrasts with the test error initially decreasing then increasing after a certain point around 15-20 predictors. Although adding more variables reduces the training error, it actually hurts the model's performance on unseen data.

The current model error at 10-20 predictor variables. is slightly lower than the competition prediction model's average error (shown in the green line above). However, the competition model's average error is lower than the error from predicting all outcomes to be just the average outcome on the entire dataset we saw prior, which was higher than 0.55. This comparison is performed on the log scale.

## Sub-problem 6: training and test error on subsets of the data (points: 5/5(extra) – graduate/undergraduate)

Perform tasks specified above (using bootstrap to estimate training and test error), but applying them only to the first 200, 500 and 1000 observations in the fund raising dataset provided.  Comment on how the behavior of training and test error across the range of model complexity changes with the change in the sample size.  For the ease of comparisons and interpretation, please make sure that all three resulting plots (error vs. number of predictors) use the same limits for the vertical (Y) axis.

```{R P1.6}
old.par <- par(mfrow=c(1,3))
for (tmpNobs in c(200,500,1000) ) {
  newDat <- myDat_quadratic[1:tmpNobs, ]
  res <- bootTrainTestErrFundraising(newDat,responseVar="contrib", nBoot=30)
  plotBootRegrErrRes(res,main=paste(tmpNobs,"observations"),ylim=c(0.1,1.0))
}
par(old.par)
```

## Sub-problem 7: using centered and scaled predictor values (points: 5/5(extra) – graduate/undergraduate)

Given the undesirable effects of having highly correlated predictors in the model (for the reasons of collinearity, variance inflation, etc.) it would be more adviseable to center and scale predictors in this dataset prior to creating higher order terms.  There is a function `scale` in R for that.  Please explore the effect of using such transformation.  You should be able to demonstrate that it does decrease correlations between predictors (including their products) while it has very little impact on the performance of the resulting models in terms of training/test error.  If you think carefully about what is necessary here, the required change could be as small as adding one (optional) call to `scale` placed strategically in the code and then compiling and comparing results with and without executing it.


**Comment:**

Please then see the other compiled file without scaling for comparison.
